{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Transformers - Part 2\n",
    "\n",
    "In this lab, you will use some of the components you wrote in the last lab to construct an entire transformer. Then you will train it on movie dialogues so, given a line from a movie, it can respond with its own line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "%pip install tensorflow-datasets==4.9.2\n",
    "import tensorflow_datasets as tfds\n",
    "%pip install pydot\n",
    "import pydot\n",
    "%conda install -c conda-forge pygraphviz\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Overview of an Encoder-Decoder Transformer Architecture\n",
    "\n",
    "Recall from the last lab that the Attention mechanism is the key component of a Transformer. However, to use Attention to learn meaningful representations of sequences and actually then predict a sequence requires us to add Dense layers around the Attention layers that will do the bulk of the learning for the model. The original Transformer paper, [Attention Is All You Need](https://arxiv.org/abs/1706.03762), implemented a Encoder-Decoder Transformer that split the model into an encoder that encodes the input sequence into a meaningful, higher-dimension, representation, and a decoder, that autoregressively (one-token at a time) generated an output sequence. OpenAI's GPT models, those that are used in ChatGPT, do not use an encoder and are decoder-only, so the input is fed directly into the decoder. In this lab, we will implement an Encoder-Decoder Transformer. An example of an encoder-decoder transformer for translation is shown below. \n",
    "\n",
    "![](./images/output_shift.png)\n",
    "\n",
    "After the input sequence is encoded by the encoder to the tensor $z$ that has shape ```(batch_size, max_length, d_model)```, $z$ conditions the decoder by serving as the keys and values for some attention layers in the decoder. Recall that the decoder takes in a sequence and outputs the same sequence shifted to the left by 1, so that the last token in the output is the \"next\" token predicted by the model. The components you implemented in the last lab are given to you here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.math.equal(x, 0)\n",
    "  mask = tf.cast(mask, tf.float32)\n",
    "  mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "  return mask\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "  max_length = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((max_length, max_length)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  look_ahead_mask = tf.maximum(look_ahead_mask, padding_mask)\n",
    "  return look_ahead_mask\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "  d_query = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(d_query)\n",
    "\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.d_model//self.num_heads))\n",
    "    inputs = tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    return inputs\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    outputs = self.dense(concat_attention)\n",
    "    return outputs\n",
    "  \n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, max_position, d_model):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "      self.pos_encoding = self.positional_encoding(max_position, d_model)\n",
    "\n",
    "  def get_angles(self, positions, inds, d_model):\n",
    "      angles = 1 / tf.pow(10000, (2 * (inds // 2)) / tf.cast(d_model, tf.float32))\n",
    "      return positions * angles\n",
    "\n",
    "  def positional_encoding(self, max_position, d_model):\n",
    "      positions = tf.range(max_position, dtype=tf.float32)[:, tf.newaxis]\n",
    "      feature_inds = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "\n",
    "      angle_rads = self.get_angles(\n",
    "          positions=positions,\n",
    "          inds=feature_inds,\n",
    "          d_model=d_model)\n",
    "      \n",
    "      sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "      cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "      pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "      pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "      return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Cornell Movie-Dialogs Corpus\n",
    "\n",
    "We will use the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) that contains over 200,000 conversations between 10,000 pairs of movie characters across over 600 movies.\n",
    "\n",
    "### Section 1.1: Loading the Dataset\n",
    "After loading roughly a quarter of the dataset, we clean the sentences by removing capitalization and any character that is not a letter or punctuation, and split the conversations into questions (preceding sentence) and answers (following sentence). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin=\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
    "                                           'movie_conversations.txt')\n",
    "\n",
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 50000\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "  # dictionary of line id to text\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    # get conversation in a list of line ID\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    for i in range(len(conversation) - 1):\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "      if len(inputs) >= MAX_SAMPLES:\n",
    "        return inputs, outputs\n",
    "  return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1000, 200):\n",
    "    print('Sample question: {}'.format(questions[i]))\n",
    "    print('Sample answer: {}'.format(answers[i]))\n",
    "    print('Sample question: {}'.format(questions[i+1]))\n",
    "    print('Sample answer: {}'.format(answers[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Tokenize the sentences\n",
    "\n",
    "Recall from the previous lab that we need to convert sentences to integers so they can be handled by the model. There are many ways to do this. The simplest way is to split a sentence by spaces so \"Tokenization is essential for text analysis.\" becomes [\"Tokenization\", \"is\", \"essential\", \"for\", \"text\", \"analysis\", \".\"] and each new word seen in the dataset is assigned a new integer. The most common and effective method for Tokenization is Byte-Pair Encoding for Subword tokens. This is an algorithm that starts from a vocabulary of letters, and iteratively merges them based on what subword (part of a word) is used most frequently. For example, after subword tokenization, \"Tokenization is essential for text analysis.\" becomes [\"Tok\", \"en\", \"ization\", \"is\", \"es\", \"sen\", \"tial\", \"for\", \"text\", \"an\", \"al\", \"ysis\", \".\"]. Notice that more frequently used subwords like \"is\", \"for\", \"text\", get their own token. Subword tokenization can greatly reduce the total number of tokens we need to represent the entire dataset. \n",
    "\n",
    "Here, we use ```tfds.features.text.SubwordTextEncoder``` to do the subword tokenization. We also need to define a special start and end token. The start token is used during inference, when the decoder needs to take in some token to start the autoregressive output. The end token tells us when to stop calling the transformer. We also need to pad our sentences so that they are all the same length. Remember the masks we made in the last lab so that the Transformer ignores these pad tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1234)\n",
    "# Create the tokenizer using tfds.deprecated.text.SubwordTextEncoder.build_from_corpus\n",
    "# Concatenate the questions and answers for the corpus_generator (which is just an array)\n",
    "# Set the target_vocab_size to 2^13\n",
    "tokenizer = None\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "# Make start_token be the integer tokenizer.vocab_size\n",
    "# and end_token to be tokenizer.vocab_size + 1\n",
    "START_TOKEN, END_TOKEN = None\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # tokenize sentence by using tokenizer.encode\n",
    "    # ensure to add the start_token to the beginning of the sentence and the\n",
    "    # end_token to the end\n",
    "    sentence1 = None\n",
    "    sentence2 = None\n",
    "    # Append the sentences to the tokenized inputs and outputs if their\n",
    "    # lengths are less than or equal to max_length\n",
    "    pass\n",
    "\n",
    "  # pad the tokenized inputs and outputs using tf.keras.preprocessing.sequence.pad_sequences\n",
    "  # make sure to use 'post' padding so that 0s are added to the end of the sequence\n",
    "  tokenized_inputs = None\n",
    "  tokenized_outputs = None\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that tokenization was done correctly \n",
    "question_tokens_solution = [8331, 38, 18, 115, 32, 3065, 19, 981, 8195, 2957, 8107, 2381, 3600, 2384, 13, 7541, 944, 6632, 8107, 46, 466, 85, 5560, 7950, 227, 3091, 3944, 131, 1460, 752, 77, 41, 6, 2117, 8175, 2, 237, 1, 8332, 0]\n",
    "answer_tokens_solution = [8331, 72, 3, 4, 180, 18, 56, 365, 40, 1086, 1692, 4214, 825, 3, 53, 15, 8, 1116, 40, 29, 1, 8332, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(questions[0])\n",
    "print(answers[0])\n",
    "assert((questions[0] == question_tokens_solution).all())\n",
    "assert((answers[0] == answer_tokens_solution).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3: Make Tensorflow dataset\n",
    "\n",
    "For our dataset, the inputs are going to be both the inputs to the encoder and the inputs to the decoder, and the outputs will be the sequence that the decoder outputs. Remember that the whole point of the look-ahead masking for the last lab, was so that we can train the model to predict the input sequence but shifted over by 1 to the left, and the masking will ensure that each output token is only based on attention from tokens earlier on in the sequence. So our decoder outputs will be our decoder inputs shifted over by 1 to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        # The encoder inputs which are the questions\n",
    "        'inputs': None,\n",
    "        # The decoder inputs which are the first 39 tokens of answers\n",
    "        'dec_inputs': None\n",
    "    },\n",
    "    {\n",
    "        # the decoder outputs which are the last 39 tokens of answers\n",
    "        'outputs': None\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE, seed=1234)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Check that dataset is correct\n",
    "first_batch = next(iter(dataset))\n",
    "encoder_inputs = first_batch[0]['inputs'] \n",
    "decoder_inputs = first_batch[0]['dec_inputs']\n",
    "decoder_outputs = first_batch[1]['outputs']\n",
    "first_encoder_inputs_solution = tf.constant([8331, 36, 224, 44, 90, 5, 85, 957, 4661, 8107, 88, 273, 7, 8332, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "first_decoder_inputs_solution = tf.constant([8331, 222, 142, 51, 69, 2, 78, 1, 8332, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "first_decoder_outputs_solution = tf.constant([222, 142, 51, 69, 2, 78, 1, 8332, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "assert((encoder_inputs[0] == first_encoder_inputs_solution).numpy().all())\n",
    "assert((decoder_inputs[0] == first_decoder_inputs_solution).numpy().all())\n",
    "assert((decoder_outputs[0] == first_decoder_outputs_solution).numpy().all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Encoder\n",
    "\n",
    "The goal of the encoder is to represent the input sequence as a sequence of embeddings that capture important information that the decoder will use to make a prediction. \n",
    "\n",
    "### Section 2.1: Encoder Layer\n",
    "\n",
    "The encoder consists of blocks of layers that are repeated multiple times. We will call each of these blocks an encoder layer. Below is a figure that summarizes what is contained in the encoder layer:\n",
    "\n",
    "![](./images/encoder.png)\n",
    "\n",
    "Implement ```encoder_layer``` using the comments. For the layers, you can define and apply a layer in one line. For example, ```out = tf.keras.layers.Dense(units=64, activation=\"relu\")(in)```\n",
    "\n",
    "When you are done, the computational graph of ```encoder_layer``` should look like the following: \n",
    "\n",
    "<img src=\"./images/encoder_layer_graph.png\" width=\"1200\"/>\n",
    "\n",
    "### Section 2.2: Encoder\n",
    "\n",
    "The encoder itself consists of multiple encoder layers, called one after another.\n",
    "\n",
    "Implement ```encoder``` using the comments. The graph should look like this: \n",
    "\n",
    "<img src=\"./images/encoder_graph.png\" width=\"1200\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  # the input of the encoder layer takes shape (batch_size, max_length, d_model)\n",
    "  # remember that input shapes ignore the batch dimension\n",
    "  # we don't need to specify max_length in the input shape because the network\n",
    "  # parameters don't depend on the length of the input anywhere\n",
    "  # thus, the sequence length dimension is None\n",
    "  # use tf.keras.Input and specify the name argument as \"inputs\"\n",
    "  inputs = None\n",
    "\n",
    "  # recall that the shape of padding_mask is (batch_size, 1, 1, max_length)\n",
    "  # since it needs to be broadcasted over the d_model and num_heads dimensions\n",
    "  # but again the sequence length dimension is None\n",
    "  # use tf.keras.Input and specify the name argument as \"padding_mask\"\n",
    "  padding_mask = None\n",
    "\n",
    "  # MultiHeadAttention with d_model, num_heads, and a name argument of \"attention\"\n",
    "  # give it the corresponding inputs, reference the class to see how the inputs are received\n",
    "  attention = None\n",
    "  \n",
    "  # add a dropout layer at rate \"dropout\" and call it on attention\n",
    "  # we use dropout on the attention output to ensure that some attention\n",
    "  # weights don't dominate the others, similar to why we used dropout in neural\n",
    "  # networks\n",
    "  # use tf.keras.layers.Dropout\n",
    "  attention = None\n",
    "\n",
    "  # adding the inputs back to the attention is a residual connection\n",
    "  # residual connections prevent the vanishing gradient problem\n",
    "  # the vanishing gradient problem is when gradients that are backpropogated\n",
    "  # get very small (close to 0) in very deep networks\n",
    "  # you can think of residual connections as helping the model to not forget things\n",
    "  attention = tf.keras.layers.Lambda(lambda x: tf.math.add(x[0], x[1]), name='add_inputs')([attention, inputs])\n",
    "\n",
    "  # Layer normalization normalizes across the feature (d_model) dimension\n",
    "  # this ensures that there are no issues as the feature distribution changes\n",
    "  # over the course of training which stabilizes and accelerates learning\n",
    "  # use tf.keras.layers.LayerNormalization with epsilon = 1e-6\n",
    "  # call it on attention\n",
    "  attention = None\n",
    "\n",
    "  # apply a dense layer to attention with \"units\" units and relu activation  \n",
    "  outputs = None\n",
    "  \n",
    "  # apply a dense layer to outputs with \"d_model\" units (and no activation)\n",
    "  outputs = None\n",
    "\n",
    "  # apply dropout to outputs with rate \"dropout\"\n",
    "  outputs = None\n",
    "\n",
    "  # another residual connection\n",
    "  outputs = tf.keras.layers.Lambda(lambda x: tf.math.add(x[0], x[1]), name='add_attention')([attention, outputs])\n",
    "\n",
    "  # apply layer normalization with epsilon 1e-6 to outputs\n",
    "  # this is another residual connection\n",
    "  outputs = None\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer = encoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_encoder_layer, to_file=\"./encoder_layer.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "  # The inputs to the encoder is an array of tokens (integers) with shape\n",
    "  # (batch_size, max_length). batch_dimension is ignored in the input shape\n",
    "  # and the sequence dimension is variable so the input shape is just (None)\n",
    "  # use tf.keras.Inputs and give the name argument \"inputs\"\n",
    "  inputs = None\n",
    "\n",
    "  # The padding mask is the same shape as in the encoder_layer\n",
    "  # give it the name argument padding_mask\n",
    "  padding_mask = None\n",
    "\n",
    "  # Recall that we convert the integer tokens to higher dimensional embedding\n",
    "  # vectors that better represent the words\n",
    "  # The embedding per token is learned.\n",
    "  # Use tf.keras.layers.Embedding with input_dim = vocab_size and\n",
    "  # output_dim = d_model. Apply it to the inputs\n",
    "  embeddings = None\n",
    "\n",
    "  # multiply embeddings by sqrt(d_model) so that the scale is consistent\n",
    "  # with attention layers later on\n",
    "  # you will need to cast d_model as a tf.float32 before taking the sqrt\n",
    "  embeddings *= None\n",
    "\n",
    "  # apply the PositionalEncoding that you defined earlier to embeddings\n",
    "  # use vocab_size for max_position and d_model for d_model\n",
    "  embeddings = None\n",
    "\n",
    "  # apply dropout with rate \"dropout\" to embeddings\n",
    "  # this regularizes the embeddings, so that the embedding transformation\n",
    "  # doesn't rely too heavily on certain elements\n",
    "  outputs = None\n",
    "\n",
    "  # call encoder_layer num_layers times give it units, d_model, num_heads, dropout\n",
    "  # and each encoder_layer should take a name argument of \"encoder_layer_i\" for\n",
    "  # the iteration number i. The inputs to the encoder_layer should be outputs\n",
    "  # and remember to give it the padding mask as well.\n",
    "  for i in range(num_layers):\n",
    "    outputs = None\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = encoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=3,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "   sample_encoder, to_file='encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Decoder\n",
    "\n",
    "The goal of the decoder is to output the input sequence shifted to left and with one new, predicted token. Here's a figure of it:\n",
    "\n",
    "<img src=\"./images/decoder.png\" width=500 />\n",
    "\n",
    "### Section 3.1: Decoder Layer\n",
    "\n",
    "The decoder should also take keys and values from the encoder output and use it in a Cross-Attention layer. Cross-Attention just means that the keys and values come from a different tensor than the queries. Remember during training, we want the $i$th output of the decoder to not perform attention with any tokens later in the sequence because during training those tokens are present. We do this with the look-ahead mask that you defined earlier. \n",
    "\n",
    "Implement ```decoder_layer```. The model graph should look like:\n",
    "\n",
    "<img src=\"./images/decoder_layer_graph.png\" width=1000 />\n",
    "\n",
    "### Section 3.1: Decoder\n",
    "\n",
    "The decoder is a stack of decoder layers. Again we convert tokens to embeddings and add positional embeddings.\n",
    "\n",
    "Implement ```decoder```. The model graph should look like:\n",
    "\n",
    "<img src=\"./images/decoder_graph.png\" width=1000 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  \n",
    "  # same as the inputs to the encoder layer\n",
    "  inputs = None\n",
    "\n",
    "  # the encoder outputs are also the shape (batch_size, max_length, d_model)\n",
    "  # we want to take them in as input so use tf.keras.Input with the same shape\n",
    "  # as inputs, and give the name argument \"encoder_outputs\"\n",
    "  enc_outputs = None\n",
    "\n",
    "  # the look_ahead_mask is shape (batch_size, 1, max_length, max_length) and\n",
    "  # it will be broadcasted over the num_heads dimension. batch_size is ignored,\n",
    "  # and since max_length is variable it is None in the input shape.\n",
    "  # give it the name \"look_ahead_mask\"\n",
    "  look_ahead_mask = None\n",
    "  \n",
    "  # define the padding mask as we did in encoder_layer\n",
    "  padding_mask = None\n",
    "\n",
    "  # perform Self-MultiHeadAttention with name attention_1 and the look_ahead_mask\n",
    "  attention1 = None\n",
    "  \n",
    "  # residual connection between attention1 and inputs\n",
    "  attention1 = tf.keras.layers.Lambda(lambda x: tf.math.add(x[0], x[1]), name='add_inputs')([attention1, inputs])\n",
    "\n",
    "  # layer normalization with epsilon 1e-6 applied to attention1\n",
    "  attention1 = None\n",
    "\n",
    "  # Cross attention using enc_outputs for the keys and values\n",
    "  # still use MultiHeadAttention, but adjust the inputs\n",
    "  # use padding_mask here because that was what was used for the encoder\n",
    "  # give it name attention_2\n",
    "  attention2 = None\n",
    "  \n",
    "  # apply dropout with rate dropout to attention2\n",
    "  attention2 = None\n",
    "  \n",
    "  # residual connection between attention2 and attention1\n",
    "  attention2 = tf.keras.layers.Lambda(lambda x: tf.math.add(x[0], x[1]), name='add_attention1')([attention2, attention1])\n",
    "\n",
    "  # layer normalization with epsilon 1e-6 applied to attention2\n",
    "  attention2 = None\n",
    "\n",
    "  # Dense layer of \"units\" units and relu activation applied to attention2\n",
    "  outputs = None\n",
    "  \n",
    "  # Dense layer of \"d_model\" units applied to outputs (no activation)\n",
    "  outputs = None\n",
    "\n",
    "  # Dropout layer of rate \"dropout\" applied to outputs\n",
    "  outputs = None\n",
    "  \n",
    "  # residual connection between outputs and attention2\n",
    "  outputs = tf.keras.layers.Lambda(lambda x: tf.math.add(x[0], x[1]), name='add_attention2')([outputs, attention2])\n",
    "\n",
    "  # layer norm with epsilon 1e-6 applied to outputs\n",
    "  outputs = None\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder_layer = decoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder_layer, to_file='decoder_layer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name='decoder'):\n",
    "  \n",
    "  # same input shape as encoder since input is a list of tokens\n",
    "  inputs = None\n",
    "\n",
    "  # same input shape for enc_outputs in decoder_layer\n",
    "  enc_outputs = None\n",
    "\n",
    "  # same input shape as look_ahead_mask in decoder_layer\n",
    "  look_ahead_mask = None\n",
    "  \n",
    "  # same input shape as padding_mask in decoder_layer\n",
    "  padding_mask = None\n",
    "\n",
    "  # same embeddings architecture from encoder\n",
    "  embeddings = None\n",
    "  embeddings *= None\n",
    "  embeddings = None\n",
    "\n",
    "  # apply dropout of rate \"dropout\" to embeddings\n",
    "  outputs = None\n",
    "\n",
    "  # call decoder_layer num_layers times with name \"decoder_layer_{i}\"\n",
    "  # decoder_layer takes 4 inputs, make sure you pass in all of them\n",
    "  for i in range(num_layers):\n",
    "    outputs = None\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = decoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=3,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder, to_file='decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Transformer\n",
    "\n",
    "Now to put everything together in the transformer. \n",
    "\n",
    "First we give the encoder the \"question\". Then we give the decoder the decoder inputs (during inference this is the answer so far) and the encoder outputs. The output of the decoder will be a sequence of embeddings with shape ```(batch_size, max_length, d_model)``` but we need tokens to actually convert the output to words so we apply a dense layer that transforms the output to be shape ```(batch_size, max_length, vocab_size)```.\n",
    "\n",
    "Implement ```transformer```. The model graph should look like:\n",
    "\n",
    "<img src=\"./images/transformer_graph.png\" width=1000 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"):\n",
    "  \n",
    "  # inputs are a sequence of tokens\n",
    "  inputs = None\n",
    "\n",
    "  # decoder inputs which are also a sequence of tokens\n",
    "  # give it the name \"dec_inputs\"\n",
    "  dec_inputs = None\n",
    "\n",
    "  # the padding mask used for the encoder\n",
    "  # use tf.keras.layers.Lambda that calls a function given some inputs\n",
    "  # use create_padding_mask for the function, and give it the output_shape\n",
    "  # that is the input_shape for it in the encoder\n",
    "  # apply the lambda layer to inputs and give it the name \"enc_padding_mask\"\n",
    "  enc_padding_mask = None\n",
    "  \n",
    "  # define a Lambda layer with create_look_ahead_mask and the appropriate\n",
    "  # output_shape and apply it to dec_inputs with name look_ahead_mask\n",
    "  look_ahead_mask = None\n",
    "  \n",
    "  # same as the enc_padding_mask but with name \"dec_padding_mask\", still applied\n",
    "  # to inputs\n",
    "  dec_padding_mask = None\n",
    "  \n",
    "  # call the encoder with all the necessary arguments \n",
    "  enc_outputs = None\n",
    "\n",
    "  # call the decoder with all the necessary arguments \n",
    "  dec_outputs = None\n",
    " \n",
    "  # apply a dense layer to transform the embeddings to logits that will be\n",
    "  # used to classify which word in the vocabulary is most likely for this token\n",
    "  # has \"vocab_size\" units and name \"ouputs\", applied to dec_outputs\n",
    "  # no activation\n",
    "  outputs = None\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = transformer(\n",
    "    vocab_size=8192,\n",
    "    num_layers=4,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_transformer, to_file='transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Training the model\n",
    "\n",
    "Although we just need to use ```model.fit``` to train the transformer given our dataset, we need to specify the loss function and a custom learning rate as used in the paper. \n",
    "\n",
    "### Section 5.1: Loss function\n",
    "\n",
    "The output of our transformer has shape ```(batch_size, max_length, vocab_size)``` which is a batch of sequences of vectors that should tell us which token in the vocabulary is most likely to be at that position in the sequence. Thus, think about this as a classification task. As we've done before, we use categorical cross-entropy loss for classification that compares a one-hot encoding of the true label to a distribution of the words given by the model. Implement ```loss_function```.\n",
    "\n",
    "### Section 5.2: Custom Learning Rate Schedule\n",
    "\n",
    "There's no reason why the learning rate needs to stay the same over the entire course of training. In the \"Attention is All You Need\" paper, they used a custom learning rate schedule. It follows the formula: \n",
    "$$\\text{learning{\\_}rate}(t, \\text{warmup{\\_}steps}) = d_{model}^{-0.5} \\min(t^{-0.5}, t(\\text{warmup{\\_}steps}^{-1.5}))$$\n",
    "\n",
    "Essentially, there is a warmup time where the learning rate starts low and then goes high. This prevents the model from losing stability when learning basic patterns early on. Then after the warmup, the learning rate gradually decreases to prevent early convergence. Implement ```CustomSchedule``` and verify that the schedule graph matches:\n",
    "\n",
    "<img src=\"./images/schedule.png\" width=400 />\n",
    "\n",
    "### Section 5.3: Train the model\n",
    "\n",
    "The accuracy should increase steadily. My final accuract after 20 epochs was 17%. It took me roughly one and a half hours to train for 20 epochs. If you have more time you should train for more though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  # y_true currently has shape (batch_size, MAX_LENGTH-1) because each\n",
    "  # sample is a sequence of tokens.\n",
    "  # reshape it to have shape (batch_size, MAX_LENGTH-1), but infer batch_size\n",
    "  # using the -1 trick for \"filling in\" the shape.\n",
    "  y_true = None\n",
    "\n",
    "  # use tf.keras.losses.SparseCategoricalCrossentropy\n",
    "  # the reason we are using the sparse version is because the vector is very\n",
    "  # large and most of the values will be near zero.\n",
    "  # specify from_logits to be true since we haven't taken a softmax\n",
    "  # set reduction to 'none' and apply it to y_true and y_pred\n",
    "  loss = None\n",
    "\n",
    "  # padding mask for the loss because we don't want to consider the\n",
    "  # loss of the pad tokens \n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    # compute the first argument of the min function\n",
    "    arg1 = None\n",
    "\n",
    "    # compute the second argument of the min function\n",
    "    arg2 = None\n",
    "\n",
    "    # return the learning rate for this step using the formula\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Inference\n",
    "\n",
    "Try out the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "loaded_model.load_weights('./model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # concatenated the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence, model):\n",
    "  prediction = evaluate(sentence, model)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict('What\\'s your favorite color?', loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The name\\'s bond, James Bond'\n",
    "for _ in range(3):\n",
    "  sentence = predict(sentence, loaded_model)\n",
    "  print('')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
