{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Transformers - Part 1\n",
    "\n",
    "ChatGPT was released in 2022 and greatly increased the exposure of Large Language Models (LLMs), and they're capabilities, to the world. In this lab, you are going to implement some of the key parts of the Transformer architecture, which is the architecture that is used for modern LLMs. In the next lab, you will use these components and finish writing an entire transformer that will generate responses to movie dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Sequence Modeling and Embeddings\n",
    "\n",
    "Previously in this course we covered neural networks and convolutional neural networks. Both can be used to extract features from data, and gradually reduce the size of the features to more easily enable tasks like classification. But what if we want the neural network to output something that looks like the data that was inputted? Specifically, sometimes we want to take in a sequence of data parametrized by time or order and output a different sequence. \n",
    "\n",
    "![](./images/sequence.png)\n",
    "\n",
    "Today, the state-of-the-art model for this is the Transformer. Before we dive into it, we will cover some basics on sequence modeling.\n",
    "\n",
    "Just like we can represent images with pixels, we can represent words as tokens. A token is an integer that represents a specific word or a part of a word. We take some fixed vocabulary size, and tokenize the entire training dataset so any word in the dataset can be represented by an integer.\n",
    "\n",
    "Just like how in a convolutional neural network we turn pixels into feature vectors that embed some information about the pixels, we convert each token to an embedding vector that is some higher-dimension representation of the word. We need to learn an embedding matrix to achieve this, and we can specify the size of the word embedding to be anything we want. Representing words as embeddings helps the transformer reason about them better. \n",
    "\n",
    "![](./images/embeddings.png)\n",
    "\n",
    "We also get the model to output a vector of the size of our vocabulary, so that after applying a softmax, we can see what's the highest probability token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Attention\n",
    "\n",
    "Attention is the core mechanism of a Transformer. This is why the original Transformer paper was called [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "When trying to reason about a sentence, it's important to consider how words in a sentence might be related. Attention is a way for the model to \"pay attention\" to certain words in relation to others. To take a sentence and then figure out what's worth paying attention to, we use the Query-Key-Value formulation.\n",
    "\n",
    "Consider the following example of image search:\n",
    "\n",
    "![](./images/query-key-value.png)\n",
    "\n",
    "We can search for what value is the most relevant to the query by comparing the query to the keys that are associated with each value. Going back to our sentence, if we want to compare words within the same sentence to determine what words are worth attending to, we use self-attention, where the queries, keys, and values all come from the same sequence. Then, we can evaluate how similar the queries and keys are, and then relatively \"amplify\" the values of the words where the queries and keys are more aligned. This allows the model to consider words that are most similar to each other over words that aren't. Mathematically, we can do this with the attention formula:\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_{query}}})V$$\n",
    "\n",
    "$Q$ is the query and has the shape ```(batch_size, seq_length, d_query)```. Just like in previous labs, we process entire batches of inputs at a time so the first dimension is the size of the batch. seq_length is the number of words in the input, which can also be thought of as the length of the sequence. d_query is the dimension of the query embedding, meaning that the query is represented by a vector of length d_query.\n",
    "\n",
    "$K$ is the key and has the shape ```(batch_size, seq_length, d_query)```. The reason d_query is used here is so when comparing keys and queries, they align on the last dimension and their similarities can be computed.\n",
    "\n",
    "$V$ is the value and has the shape ```(batch_size, seq_length, d_value)```. d_value is not necessarily the same as d_query, because we might want to represent the values as vectors of a different size. However, in this lab, they happen to be the same.\n",
    "\n",
    "Matrix multiplying $Q$ and $K^T$ effectively gives us the dot product of every query vector with every key vector. Recall that the dot product tells us how close two vectors align, and can be thought of as a quantitative measure for how similar two vectors are. Thus, $QK^T_{ij}$ tells us how similar query $i$ is to key $j$. We want this similarity to be normalize, so we use cosine similarity, which is why we divide by $\\sqrt{d_{query}}$. We apply softmax to each row to get the attention map. Then, row $i$ of the attention map sums to 1 and is a weighting on how similar word $i$ is to every the word in the input sequence. \n",
    "\n",
    "![](./images/attention.png)\n",
    "\n",
    "Implement ```scaled_dot_product_attention_without_mask``` following the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_without_mask(query, key, value):\n",
    "  #matrix multiply Q and K^T\n",
    "  matmul_qk = None\n",
    "\n",
    "  # get d_query and cast it as tf.float32\n",
    "  d_query = None\n",
    "\n",
    "  # compute the logits of the attention map\n",
    "  # remember to divide by sqrt(d_query)\n",
    "  logits = None\n",
    "\n",
    "  # apply softmax using tf.nn.softmax to the last axis of logits\n",
    "  attention_weights = None\n",
    "\n",
    "  # apply the attention map to V\n",
    "  output = None\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scaled_dot_product_attention_without_mask\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "d_embedding = 8\n",
    "d_query = 4\n",
    "d_key = 4\n",
    "d_value = 4\n",
    "\n",
    "input = tf.random.uniform((batch_size, seq_length, d_embedding))\n",
    "\n",
    "w_Q = tf.random.uniform((d_embedding, d_query))\n",
    "w_K = tf.random.uniform((d_embedding, d_key))\n",
    "w_V = tf.random.uniform((d_embedding, d_value))\n",
    "\n",
    "Q = tf.matmul(input, w_Q)\n",
    "K = tf.matmul(input, w_K)\n",
    "V = tf.matmul(input, w_V)\n",
    "\n",
    "attn_output = scaled_dot_product_attention_without_mask(Q, K, V)\n",
    "\n",
    "attn_output_solution = tf.constant(\n",
    "  [[[2.6964214, 1.8722955, 2.0356367, 1.7275454],\n",
    "  [2.7064402, 1.8726325, 2.0396535, 1.7393928],\n",
    "  [2.7179503, 1.8745576, 2.0401611, 1.7443812]],\n",
    "\n",
    " [[2.1098378, 1.5404547, 1.986176,  1.5552578],\n",
    "  [2.1509454, 1.5733178, 2.0266113, 1.5916135],\n",
    "  [2.138824,  1.5617981, 2.013477,  1.5796201]]])\n",
    "\n",
    "print(attn_output)\n",
    "assert(tf.norm(attn_output_solution - attn_output) < 1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Masking\n",
    "\n",
    "Sometimes, we don't want our transformer to do anything anything with certain tokens in the input. Before taking the softmax in the attention mechanism, we set any part of the attention map we don't want the transformer to consider to negative infinity. \n",
    "\n",
    "### Section 2.1: Padding Masking\n",
    "\n",
    "When we define inputs or outputs for the transformer, they'll have a sequence length of ```max_length```. So if we input a sentence that has less tokens than max_length, we will pad the rest of the sequence with a special pad token represented by 0. In the attention mechanism, we want to ignore these pad tokens. \n",
    "\n",
    "Before we take the softmax to make the attention map, we will have shape ```(batch_size, num_heads, max_length, max_length)``` (we will describe num_heads later in multi-head attention). \n",
    "\n",
    "We want to create a mask where every element of the mask that is 1 should become negative infinity and every element of the mask that is 0 will stay the same. \n",
    "\n",
    "Thus, we will create a mask of shape ```(batch_size, 1, 1, max_length)``` which will broadcast over the second and third dimension of ```(batch_size, num_heads, max_length, max_length)```. \n",
    "\n",
    "Implement ```create_padding_mask``` using the comments.\n",
    "\n",
    "### Section 2.2: Look Ahead Masking\n",
    "\n",
    "The transformer we will train is autoregressive. \n",
    "\n",
    "This means it predicts the next token from the tokens its already predicted in the past. During inference (generation), we give the model a special start token, and then in a loop, it predicts the next words, appends it to the current sentence, and repeats, until it predicts a special end token. \n",
    "\n",
    "During training we already have the output which is the input but shifted to the left by one token (see image below). If the decoder takes in $n$ tokens, it must also output $n$ tokens.\n",
    "\n",
    "![](./images/output_shift.png)\n",
    "\n",
    "However, during training, we don't want to compute attention between the $i$-th token and any token $> i$, i.e. in the future. That way, the $i$-th output token is what the transformer would output if it only saw (and applied attention to) the first $i$ input tokens. \n",
    "\n",
    "To prevent the transformer from \"looking ahead\" in the attention, we use a mask that is 1 where we want the transformer to ignore the attention map and 0 where its alright to apply attention. Specifically, the mask is:\n",
    "$$m_{ij} =\n",
    "  \\begin{cases} \n",
    "      1 & i < j \\\\\n",
    "      0 & i \\geq j \\\\ \n",
    "   \\end{cases}\\\\$$\n",
    "and has the shape ```(batch_size, 1, max_length, max_length)``` as it will broadcast over the num_heads dimension. \n",
    "Since the output of softmax approaches 0 as the input approaches negative infinity, we multiply the mask by some large negative number.\n",
    "\n",
    "![](./images/look_ahead_masking.png)\n",
    "\n",
    "We also want to make sure that pad tokens are masked out in this mask as well. \n",
    "\n",
    "Implement ```create_look_ahead_mask``` using the comments.\n",
    "\n",
    "### Section 2.3: Attention with Mask\n",
    "\n",
    "Now that we have these two kinds of masks, we need to change our attention method to use the mask. \n",
    "\n",
    "Remember that we want to set values of the mask that are 1 to negative infinity, before the attention map is passed to the softmax. That way, the masked values will become zero after the softmax and the transformer won't \"pay any attention\" to them. \n",
    "\n",
    "Implement ```scaled_dot_product_attention``` using the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  # create the padding mask which is a boolean tensor\n",
    "  # 1 where the tokens are 0, 0 otherwise\n",
    "  # Hint: tf.math.equal\n",
    "  mask = None\n",
    "\n",
    "  # cast the mask as tf.float32\n",
    "  mask = None\n",
    "\n",
    "  # the mask is currently of shape (batch_size, max_length)\n",
    "  # give it the shape (batch_size, 1, 1, max_length)\n",
    "  mask = None\n",
    "\n",
    "  return mask\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "  # x is of shape (batch_size, max_length)\n",
    "  # get max_length\n",
    "  max_length = None\n",
    "\n",
    "  # create the look ahead mask using tf.linalg.band_part and tf.ones\n",
    "  # refer to the formula for the mask above: want only 1s above the diagonal\n",
    "  look_ahead_mask = None\n",
    "\n",
    "  # use create_padding_max to get the padding mask of x\n",
    "  padding_mask = None\n",
    "\n",
    "  # combine the look_ahead_mask and the padding_mask\n",
    "  # Hint: use tf.maximum\n",
    "  look_ahead_mask = None\n",
    "\n",
    "  return look_ahead_mask\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  #matrix multiply Q and K^T\n",
    "  matmul_qk = None\n",
    "\n",
    "  # get d_query and cast it as tf.float32\n",
    "  d_query = None\n",
    "\n",
    "  # compute the logits of the attention map\n",
    "  # remember to divide by sqrt(d_query)\n",
    "  logits = None\n",
    "\n",
    "  # multiply the mask by a large negative number (-1e9) and add it to logits\n",
    "  # this effectively makes masked values negative infinity\n",
    "  if mask is not None:\n",
    "    logits += None\n",
    "\n",
    "  # apply softmax using tf.nn.softmax to the last axis of logits\n",
    "  attention_weights = None\n",
    "\n",
    "  # apply the attention map to V\n",
    "  output = None\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test create_padding_mask\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "batch_size = 1\n",
    "max_length = 5\n",
    "\n",
    "input = np.random.randint(1, 10, (batch_size, max_length))\n",
    "input[0][2] = 0\n",
    "input = tf.convert_to_tensor(input)\n",
    "\n",
    "padding_mask = create_padding_mask(input)\n",
    "print(padding_mask)\n",
    "\n",
    "padding_mask_solution = tf.constant([[0, 0, 1, 0, 0]], dtype=tf.float32)\n",
    "\n",
    "assert(tf.norm(padding_mask - padding_mask_solution) < 1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test create_look_ahead_mask\n",
    "look_ahead_mask = create_look_ahead_mask(input)\n",
    "print(look_ahead_mask)\n",
    "\n",
    "look_ahead_mask_solution = tf.constant(\n",
    "[[[[0, 1, 1, 1, 1],\n",
    "   [0, 0, 1, 1, 1],\n",
    "   [0, 0, 1, 1, 1],\n",
    "   [0, 0, 1, 0, 1],\n",
    "   [0, 0, 1, 0, 0]]]], dtype=tf.float32)\n",
    "\n",
    "assert(tf.norm(look_ahead_mask - look_ahead_mask_solution) < 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scaled_dot_product_attention\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "d_embedding = 8\n",
    "d_query = 2\n",
    "d_key = 2\n",
    "d_value = 2\n",
    "\n",
    "input = tf.random.uniform((batch_size, seq_length, d_embedding))\n",
    "\n",
    "w_Q = tf.random.uniform((d_embedding, d_query))\n",
    "w_K = tf.random.uniform((d_embedding, d_key))\n",
    "w_V = tf.random.uniform((d_embedding, d_value))\n",
    "\n",
    "Q = tf.matmul(input, w_Q)\n",
    "K = tf.matmul(input, w_K)\n",
    "V = tf.matmul(input, w_V)\n",
    "\n",
    "attn_output = scaled_dot_product_attention(Q, K, V, look_ahead_mask_solution)\n",
    "\n",
    "attn_output_solution = tf.constant(\n",
    "  [[[[1.9943544, 1.3863453],\n",
    "   [1.902195,  1.5321324],\n",
    "   [1.9024689, 1.5316987],\n",
    "   [1.8435459, 1.4898659],\n",
    "   [1.8339779, 1.5064971]],\n",
    "  [[1.8489847, 1.0564694],\n",
    "   [1.7953603, 1.199664 ],\n",
    "   [1.7964938, 1.1966374],\n",
    "   [2.4567406, 1.6503923],\n",
    "   [2.383724,  1.6358945]]]])\n",
    "\n",
    "print(attn_output)\n",
    "assert(tf.norm(attn_output_solution - attn_output) < 1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Multi-Head Attention\n",
    "\n",
    "The original transformer paper, \"Attention Is All You Need\", proposed to use multi-head attention (MHA). \n",
    "\n",
    "For every Q, K, and V, the attention is split among ```num_heads``` heads. This allows for better expression and attending to multiple things at once. \n",
    "\n",
    "HMA takes in the sequence used for the queries, keys, and values (note that the sequence used for the queries can be different than the sequence used for the keys and values). \n",
    "\n",
    "Let's say $q$ is the query inputs, $k$ is the key inputs, and $v$ is the value inputs and each of them has shape ```(batch_size, max_length, d_embedding)```. \n",
    "\n",
    "We apply learned weight matrices, $W_Q$, $W_K$, $W_V$, of shapes ```(d_embedding, d_query)```, ```(d_embedding, d_key)```, ```(d_embedding, d_value)``` to $q, k, v$ so we get $Q = qW_Q, K = kW_k, V = kW_v$ with shapes ```(batch_size, max_length, d_query)```, ```(batch_size, max_length, d_key)```, ```(batch_size, max_length, d_value)```. \n",
    "\n",
    "In Multi-Head attention, we split $Q, K, V$ along the $d_{query}/d_{key}/d_{value}$ dimension into ```num_heads``` different pieces (heads). So after we split $Q$, it will have shape ```(batch_size, num_heads, max_length, d_key//num_heads)```. Notice that the number of elements didn't change since we divided the last dimension by the number of heads. \n",
    "\n",
    "After we perform attention on the split $Q, K, V$ (which works due to broadcasting), we concatenate the outputs back together to get the shape ```(batch_size, max_length, d_value)```. We finish with one last dense layer.\n",
    "\n",
    "Refer to the image below for a visual explanation. Implement parts of the class ```MultiHeadAttention```.\n",
    "\n",
    "![](./images/multi-head-attention.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    # number of heads we will split Q, K, V into\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    # d_model = d_query = d_key = d_value\n",
    "    self.d_model = d_model\n",
    "\n",
    "    # d_model needs to be divisible by the number of heads so the split is even\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # We will represent W_Q, W_K, W_V as dense layers because we want to learn them.\n",
    "    # Recall that dense layers are specified by the number of outputs.\n",
    "    # Since W_Q has shape (d_embedding, d_model), specify the number of units for\n",
    "    # each dense layer accordingly.\n",
    "    self.query_dense = None\n",
    "    self.key_dense = None\n",
    "    self.value_dense = None\n",
    "\n",
    "    # MHA has a final dense layer applied to the output of attention\n",
    "    # the number of outputs is also d_model for this.\n",
    "    self.dense = None\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    # Inputs has shape (batch_size, max_length, d_model)\n",
    "    # Reshape it to have shape (batch_size, max_length, num_heads, d_model//num_heads)\n",
    "    # Notice that we don't explicitly have max_length, but that you can use -1\n",
    "    # to \"fill in\" missing dimensions when reshaping\n",
    "    inputs = None\n",
    "    \n",
    "    # Transpose inputs to have shape (batch_size, num_heads, max_length, d_model//num_heads)\n",
    "    inputs = None\n",
    "\n",
    "    return inputs\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # call the corresponding dense layer on the inputs for query, key, and value\n",
    "    query = None\n",
    "    key = None\n",
    "    value = None\n",
    "\n",
    "    # use the split_heads method to split the query, key and value\n",
    "    query = None\n",
    "    key = None\n",
    "    value = None\n",
    "\n",
    "    # call scaled_dot_product_attention, remember to use the mask\n",
    "    scaled_attention = None\n",
    "\n",
    "    # currently scaled_attention has shape (batch_size, num_heads, max_length, d_model//num_heads)\n",
    "    # transpose it to be (batch_size, max_length, num_heads, d_model//num_heads)\n",
    "    scaled_attention = None\n",
    "\n",
    "    # concatenate the heads so that scaled_attention has shape (batch_size, max_length, d_model)\n",
    "    # note that you can just use tf.reshape for this\n",
    "    concat_attention = None\n",
    "\n",
    "    # call the final dense layer on concat_attention\n",
    "    outputs = None\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Multi Head Attention\n",
    "tf.keras.utils.set_random_seed(1234)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "\n",
    "input_tokens = np.random.randint(1, 10, (batch_size, seq_length))\n",
    "input_tokens[0][2] = 0\n",
    "input_tokens = tf.convert_to_tensor(input_tokens)\n",
    "print(input_tokens)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(input_tokens)\n",
    "\n",
    "input_embeddings = tf.keras.layers.Embedding(10, d_model)(input_tokens)\n",
    "\n",
    "input = {\"query\": input_embeddings, \"key\": input_embeddings, \"value\": input_embeddings, \"mask\": look_ahead_mask}\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "mha_output = mha(input)\n",
    "print(mha_output)\n",
    "\n",
    "mha_output_solution = tf.constant(\n",
    "[[[ 0.02514083,  0.00977693, -0.00337918,  0.00960296],\n",
    "  [ 0.01701181,  0.01394134, -0.01645077, -0.01413145],\n",
    "  [ 0.0170159,   0.0139411,  -0.01644699, -0.01411536]],\n",
    "\n",
    " [[ 0.01959381,  0.00368656, -0.00541761,  0.0188225 ],\n",
    "  [ 0.00100767, -0.00967453,  0.00965437,  0.01981302],\n",
    "  [ 0.00328764,  0.0039839,  -0.00588427, -0.01156338]]], dtype=tf.float32)\n",
    "\n",
    "assert(tf.norm(mha_output_solution - mha_output) < 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Positional Encoding\n",
    "\n",
    "Attention is permutation invariant, meaning that no matter what order the words are in, the similarities between the words won't be different. But the order of the words in a sentence matters a lot! How can we adjust the inputs to the transformer so that the attention mechanism considers the order of the words? We can use positional embeddings.\n",
    "\n",
    "When constructing our input to the transformer, we apply a learned embedding layer to the tokens to turn the tokens into feature vectors. Then we add a positional embedding, element-wise, to each feature vector that should help give the feature vector a relative order to the other feature vectors. Then, words that are closer together will be considered more similar by the attention mechanism.\n",
    "\n",
    "The formula for calculating positional encoding is:\n",
    "\n",
    "$$PE_{t, 2i} = sin(t/10000^{2i/d_{model}})$$\n",
    "$$PE_{t, 2i+1} = cos(t/10000^{2i/d_{model}})$$\n",
    "\n",
    "I won't go into detail here, but to understand why this formula achieves what we want, please read [this article.](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition)\n",
    "\n",
    "Implement the ```PositionalEncoding``` class. If you implement it correctly, your graph visualizing the positional encoding should look like this:\n",
    "\n",
    "![](./images/positional_encoding.png)\n",
    "\n",
    "Notice how each row of the graph, demonstrating the positional encoding for a certain position, can be differentiated from rows far away. Adding this to the input sequence gives each feature vector a relative order. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, max_position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(max_position, d_model)\n",
    "\n",
    "  def get_angles(self, positions, inds, d_model):\n",
    "    # compute the angle that is input to either sine or cosine later\n",
    "    # make sure you cast d_model as a tf.float32 in the expression\n",
    "    angles = None\n",
    "    return positions * angles\n",
    "\n",
    "  def positional_encoding(self, max_position, d_model):\n",
    "    # get the range of positions that we need to compute positional encodings for\n",
    "    # use tf.range and ensure it has type tf.float32\n",
    "    # also add a dimension so the shape is (max_position, 1), this will be useful for broadcasting\n",
    "    positions = None\n",
    "    \n",
    "    # get the range of feature indicies\n",
    "    # note that the feature vector has length d_model\n",
    "    # use tf.range and ensure it has type tf.float32\n",
    "    # also add a dimension so the shape is (1, d_model), this will be useful for broadcasting\n",
    "    feature_inds = None\n",
    "\n",
    "    # get the angles using the get_angles method\n",
    "    angle_rads = None\n",
    "    \n",
    "    # apply sin to every even index in the array\n",
    "    sines = None\n",
    "\n",
    "    # apply cos to every odd index in the array\n",
    "    cosines = None\n",
    "\n",
    "    # concatenate sines and cosines along the last (feature) dimension\n",
    "    pos_encoding = None\n",
    "\n",
    "    # add an extra first dimension so the shape is (1, max_position, d_model)\n",
    "    # this will be useful for broadcasting across the batch\n",
    "    pos_encoding = None\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph positional encodings\n",
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, we'll take all the components you built here, and put them all into a transformer architecture that you will use to train a transformer that can respond to movie dialogues. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
